{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, learning_rate, num_layers, size_layer):\n",
    "\t\t\n",
    "        self.num_layers = num_layers\n",
    "        self.size_layer = size_layer\n",
    "        \n",
    "        rnn_cells = tf.nn.rnn_cell.LSTMCell(size_layer, state_is_tuple = False)\n",
    "        \n",
    "        self.rnn_cells =  tf.nn.rnn_cell.MultiRNNCell([rnn_cells] * num_layers, state_is_tuple = False)\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, (None, None, 1))\n",
    "        self.Y = tf.placeholder(tf.float32, (None, None, 1))\n",
    "        \n",
    "        self.net_last_state = np.zeros((num_layers * 2 * size_layer))\n",
    "        \n",
    "        self.hidden_layer = tf.placeholder(tf.float32, shape=(None, num_layers * 2 * size_layer))\n",
    "        \n",
    "        self.outputs, self.last_state = tf.nn.dynamic_rnn(self.rnn_cells, self.X, initial_state = self.hidden_layer, dtype = tf.float32)\n",
    "        \n",
    "        self.rnn_W = tf.Variable(tf.random_normal((size_layer, 1)))\n",
    "        self.rnn_B = tf.Variable(tf.random_normal((1,)))\n",
    "            \n",
    "        # linear dimension for x in (Wx + B)\n",
    "        outputs_reshaped = tf.reshape(self.outputs, [-1, size_layer])\n",
    "        \n",
    "        y_batch_long = tf.reshape(self.Y, [-1, 1])\n",
    "\t\t\n",
    "        # y = Wx + B\n",
    "        self.logits = (tf.matmul(outputs_reshaped, self.rnn_W) + self.rnn_B)\n",
    "        \n",
    "        self.cost = tf.reduce_mean(tf.square(y_batch_long - self.logits))\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(self.cost)\n",
    "\t\t\n",
    "    def step(self, sess, x, init_zero_state = True):\n",
    "\t\t\n",
    "        # Reset the initial state of the network.\n",
    "        if init_zero_state:\n",
    "            init_value = np.zeros((self.num_layers * 2 * self.size_layer,))\n",
    "        else:\n",
    "            init_value = self.net_last_state\n",
    "        \n",
    "        # we want to get the constant in our output layer, same size as dimension input\n",
    "        probs, next_lstm_state = sess.run([self.logits, self.last_state], feed_dict={self.X:[x], self.hidden_layer:[init_value]})\n",
    "\n",
    "        self.net_last_state = next_lstm_state[0]\n",
    "\n",
    "        return probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
